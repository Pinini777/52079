{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEjLaceFXNj2t1F7PjDFRF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pinini777/52079/blob/main/SemanticSearch_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "t1HL7E40AViR"
      },
      "outputs": [],
      "source": [
        "# %%capture evita que la salida ensucie la pantalla, solo muestra errores si los hay.\n",
        "%%capture\n",
        "!pip install sentence-transformers torch numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usaremos sentence-transformers al principio porque es un wrapper **\"Pythonic\"** excelente sobre Hugging Face, ideal para entender el concepto de Embedding sin pelear con tokenizers manuales todav√≠a."
      ],
      "metadata": {
        "id": "hatEXCt-S9Gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from typing import List, Dict, Any, Union\n",
        "\n",
        "class SemanticEngine:\n",
        "    \"\"\"\n",
        "    Motor de b√∫squeda sem√°ntica basado en Embeddings vectoriales.\n",
        "    Utiliza modelos Transformer para convertir texto a vectores en R^n.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
        "        \"\"\"\n",
        "        Inicializa el motor.\n",
        "\n",
        "        Args:\n",
        "            model_name (str): ID del modelo en Hugging Face.\n",
        "                              'all-MiniLM-L6-v2' es ligero y eficiente (384 dimensiones).\n",
        "        \"\"\"\n",
        "        # Verificaci√≥n de Hardware: Esto es CR√çTICO en producci√≥n.\n",
        "        # Si hay GPU, la usamos. Si no, CPU.\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        print(f\"üöÄ Iniciando Engine en dispositivo: {self.device}\")\n",
        "\n",
        "        # Carga del modelo\n",
        "        # Almacenamos el modelo como atributo privado (encapsulamiento)\n",
        "        try:\n",
        "            self._model = SentenceTransformer(model_name, device=self.device)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error al cargar el modelo {model_name}: {e}\")\n",
        "\n",
        "        # Aqu√≠ guardaremos nuestra \"Base de Datos\" vectorial temporalmente (en RAM)\n",
        "        # En el futuro, esto se reemplaza por una Vector DB (ej: ChromaDB o Pinecone)\n",
        "        self._knowledge_base: List[Dict[str, Any]] = []\n",
        "        self._embeddings_matrix: Union[np.ndarray, None] = None\n",
        "\n",
        "    def add_documents(self, documents: List[str]) -> None:\n",
        "        \"\"\"\n",
        "        Recibe una lista de textos (strings), los vectoriza y almacena.\n",
        "        \"\"\"\n",
        "        # PASO 1: Guardar texto crudo (Metadata)\n",
        "        # Iteramos sobre la lista de documentos y guardamos cada uno en un diccionario.\n",
        "        # Usamos diccionarios {'text': doc} por si a futuro queremos agregar 'fecha', 'autor', etc.\n",
        "        for doc in documents:\n",
        "            self._knowledge_base.append({\"text\": doc})\n",
        "\n",
        "        # PASO 2: Generar Embeddings (La Magia Matem√°tica)\n",
        "        # No hacemos un bucle for aqu√≠. Pasamos la lista ENTERA.\n",
        "        # La librer√≠a optimiza esto en lotes (batches) para la CPU/GPU.\n",
        "        # convert_to_numpy=True nos asegura que devuelve un Array (matriz), no una lista.\n",
        "        new_embeddings = self._model.encode(documents, convert_to_numpy=True)\n",
        "\n",
        "        # PASO 3: Gesti√≥n de la Matriz en Memoria\n",
        "        if self._embeddings_matrix is None:\n",
        "            # Si es la primera vez, la matriz es igual a los nuevos embeddings\n",
        "            self._embeddings_matrix = new_embeddings\n",
        "        else:\n",
        "            # Si ya ten√≠amos datos, \"concatenamos\" la nueva matriz debajo de la vieja.\n",
        "            # axis=0 significa \"agregar filas\" (verticalmente).\n",
        "            self._embeddings_matrix = np.vstack((self._embeddings_matrix, new_embeddings))\n",
        "\n",
        "        # Feedback para el desarrollador\n",
        "        print(f\"‚úÖ Se procesaron {len(documents)} documentos.\")\n",
        "        print(f\"üìä Dimensi√≥n de la Base de Datos Vectorial: {self._embeddings_matrix.shape}\")\n",
        "\n",
        "    def _cosine_similarity(self, vec_a: np.ndarray, vec_b: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Calcula la similitud coseno entre dos vectores.\n",
        "        \"\"\"\n",
        "        #1. Producto Punto(Dot Product)\n",
        "        #En Algebra: A . B\n",
        "        #En Numpy: np.dot(A,B)\n",
        "        dot_product = np.dot(vec_a, vec_b)\n",
        "        #2. Norma(Magnitud del vector)\n",
        "        # En Algebra: ||A||\n",
        "        #En Numpy: np.linalg.norm(A)\n",
        "        norm_a = np.linalg.norm(vec_a)\n",
        "        norm_b = np.linalg.norm(vec_b)\n",
        "\n",
        "        if norm_a == 0 or norm_b == 0:\n",
        "          return 0.0\n",
        "\n",
        "        return dot_product / (norm_a * norm_b)\n",
        "\n",
        "    def search(self, query: str, top_k: int = 3) -> List[str]:\n",
        "        \"\"\"\n",
        "        Busca los documentos m√°s similares a la query.\n",
        "        \"\"\"\n",
        "        # 1. Vectorizar la consulta (Query Embedding)\n",
        "        # El modelo convierte el texto de busqueda al mismo espacio vectorial\n",
        "        query_vector = self._model.encode([query], convert_to_numpy= True)[0]\n",
        "\n",
        "        # 2. Calcular similitudes\n",
        "        # Aca se guardan tuplas: (indice, puntaje_similitud)\n",
        "        similarities = []\n",
        "        #Iteramos sobre nuestra matriz de vectores guardados\n",
        "        for idx, doc_vector in enumerate(self._embeddings_matrix):\n",
        "            score = self._cosine_similarity(query_vector, doc_vector)\n",
        "            similarities.append((idx, score))\n",
        "\n",
        "        # 3. Ordenar resultados (ranking)\n",
        "        # Sorted ordena listas, key=lambda x: x[1] le dice \"ordena basandote en el puntaje\"\n",
        "        # reverse=Truepara que el mayor puntaje(mas similar) quede primero.\n",
        "        similarities = sorted(similarities, key = lambda x: x[1], reverse=True)\n",
        "        # 4. Recuperar los textos originales\n",
        "        results = []\n",
        "        # CORRECCI√ìN 1: Aseg√∫rate de que las variables del bucle coincidan (idx, score)\n",
        "        for idx, score in similarities[:top_k]:\n",
        "            # CORRECCI√ìN 2: _knowledge_base todo en min√∫sculas\n",
        "            original_text = self._knowledge_base[idx]['text']\n",
        "            results.append(f\"Similitud: {score:.4f} | Texto: {original_text}\")\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "9dV9oJnzjCii"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Instanciamos nuestro motor (Esto descargar√° el modelo la primera vez)\n",
        "engine = SemanticEngine()\n",
        "\n",
        "# 2. Le damos \"conocimiento\" (Documentos desordenados)\n",
        "textos_prueba = [\n",
        "    \"El perro es el mejor amigo del hombre\",  # Concepto: Mascota/Perro\n",
        "    \"La inteligencia artificial est√° revolucionando el mundo\", # Concepto: Tecnolog√≠a\n",
        "    \"Me encanta comer pizza con mucho queso\", # Concepto: Comida\n",
        "    \"Los gatos son animales muy independientes\", # Concepto: Mascota/Gato\n",
        "    \"Python es un lenguaje de programaci√≥n excelente\", # Concepto: Tecnolog√≠a/Coding\n",
        "]\n",
        "\n",
        "engine.add_documents(textos_prueba)\n",
        "\n",
        "# 3. Hacemos b√∫squedas SEM√ÅNTICAS (Nota que no uso las palabras exactas)\n",
        "print(\"\\n--- PRUEBA 1: Buscando 'animal que ladra' ---\")\n",
        "# La palabra \"ladra\" NO est√° en los textos, pero el modelo sabe que perro y ladrar est√°n relacionados.\n",
        "resultados = engine.search(\"animal que ladra\", top_k=2)\n",
        "for r in resultados: print(r)\n",
        "\n",
        "print(\"\\n--- PRUEBA 2: Buscando 'algo de inform√°tica' ---\")\n",
        "resultados = engine.search(\"algo de inform√°tica\", top_k=2)\n",
        "for r in resultados: print(r)"
      ],
      "metadata": {
        "id": "w0j1l2alMg4B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e5364c4-a195-461f-dcf3-04f7aa56ff65"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Iniciando Engine en dispositivo: cpu\n",
            "‚úÖ Se procesaron 5 documentos.\n",
            "üìä Dimensi√≥n de la Base de Datos Vectorial: (5, 384)\n",
            "\n",
            "--- PRUEBA 1: Buscando 'animal que ladra' ---\n",
            "Similitud: 0.5776 | Texto: Los gatos son animales muy independientes\n",
            "Similitud: 0.3675 | Texto: Me encanta comer pizza con mucho queso\n",
            "\n",
            "--- PRUEBA 2: Buscando 'algo de inform√°tica' ---\n",
            "Similitud: 0.4483 | Texto: La inteligencia artificial est√° revolucionando el mundo\n",
            "Similitud: 0.4340 | Texto: El perro es el mejor amigo del hombre\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "=============================================================================\n",
        "RESUMEN T√âCNICO: MOTOR DE B√öSQUEDA SEM√ÅNTICA (PROYECTO ALPHA)\n",
        "=============================================================================\n",
        "\n",
        "1. ARQUITECTURA:\n",
        "   - Implementaci√≥n de un sistema de \"Dense Retrieval\" (Recuperaci√≥n Densa).\n",
        "   - Patr√≥n de dise√±o orientado a objetos (Clase SemanticEngine) con principios\n",
        "     de encapsulamiento para el modelo y la base de conocimiento.\n",
        "\n",
        "2. MATEM√ÅTICA APLICADA:\n",
        "   - Transformaci√≥n de texto no estructurado a Vectores en R^384 (Embeddings).\n",
        "   - C√°lculo de similitud mediante Similitud del Coseno (Cosine Similarity).\n",
        "   - Uso de √Ålgebra Lineal (Producto Punto y Normas L2) sobre matrices NumPy\n",
        "     para ranking de relevancia.\n",
        "\n",
        "3. STACK TECNOL√ìGICO:\n",
        "   - Python 3.10+ (Type Hinting).\n",
        "   - PyTorch (Backend de tensores).\n",
        "   - Sentence-Transformers (Modelo: all-MiniLM-L6-v2).\n",
        "   - NumPy (Operaciones vectoriales de alto rendimiento).\n",
        "\n",
        "4. RESULTADO:\n",
        "   - Capacidad de \"Semantic Search\": El motor recupera documentos basados en\n",
        "     el significado conceptual y no solo en coincidencia exacta de palabras clave."
      ],
      "metadata": {
        "id": "q0tzJ8G9cFpg"
      }
    }
  ]
}